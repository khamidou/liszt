#!/usr/bin/env python
# This is the daemon that actually does the network operations.
# This is how it works : the ui program (liszt) creates json files corresponding to requests in the ~/.liszt/delayed directory.
# The daemon periodically scans this folder to sends the requests to the server.
# The format of a request file is :
#
# {"request": request-type, "listname" : ___, "content" : {"title" : ___, "done" : ___, "date" : ___ }}
# with request-type being : 	"create-entry", "update-entry", "delete-entry",
#				"create-list", "update-list", "delete-list".


import sys
import os
from datetime import datetime
import urllib, urllib2, cookielib
from urllib import quote
from urllib2 import HTTPError, URLError
import subprocess
from subprocess import Popen
import fnmatch
import tempfile
import thread
import time
import ConfigParser

try:
    import json 
except ImportError:
    import simplejson as json                                                                                                                         


base = "http://soundbush.com:8080/api/v1"

conf = ConfigParser.RawConfigParser()
conf.read(os.path.expanduser('~/.liszt/config'))
username = conf.get('Credentials', 'username')
password = conf.get('Credentials', 'password')


class LisztSession:
    def __init__(self):

        self.jar = cookielib.LWPCookieJar()
        if os.path.isfile(os.path.expanduser('~/.liszt/cookiejar')):
            self.jar.load(os.path.expanduser('~/.liszt/cookiejar'))

        self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(self.jar))
        urllib2.install_opener(self.opener)

    def login(self, username, password):
        d = {'username' : username, 'password': password}
        ed = urllib.urlencode(d)
        
        while True:
            try:
                urllib2.urlopen(base + "/login/", ed)
                return
            except URLError, HTTPError:
                print "Unable to log you in"
                time.sleep(5 * 60) # Try to reconnect every five minutes.
                continue

    def create_list(self, listname):
        try:
            urllib2.urlopen(base + "/create/" + quote(listname) + "/")
        except HTTPError:
            bailout("Unable to create the list " + listname)

    def create_list_entry(self, listname, entry):
        try:
            urllib2.urlopen(base + "/create/" + quote(listname) + "/" + quote(entry) + "/")
        except HTTPError:
            bailout("Unable to create the list entry in list " + listname)

    def get_lists_list(self):
        try:
            req = urllib2.urlopen(base + "/getlists/")
        except HTTPError:
            bailout("Unable to get a list of the available lists")
            
        return json.load(req)

    def get_list(self, listname):
        ln = self.glob_listname(listname)

        try:
            req = urllib2.urlopen(base + "/get/" + quote(ln) + "/")
        except HTTPError:
            bailout("Unable to open the list " + listname)

        return json.load(req)

    def get_entry(self, listname, index):
        ln = self.glob_listname(listname)

        try:
            req = urllib2.urlopen(base + "/get/" + quote(ln) + "/" + str(index) + "/")
        except HTTPError:
            bailout("Unable to get entry " + str(index) + " from list " + listname)

        return json.load(req)

    def remove_list(self, listname):
        try:
            urllib2.urlopen(base + "/remove/" + quote(listname) + "/")
        except HTTPError:
            bailout("Unable to remove the list " + listname)

    def remove_list_entry(self, listname, index):
        try:
            urllib2.urlopen(base + "/remove/" + quote(listname) + "/" + quote(index) + "/")
        except HTTPError:
            bailout("Unable to remove the entry " + index + "from list " + listname)

    def update_list_entry(self, listname, index, contents):
        ec = urllib.urlencode(contents)
        try:
            urllib2.urlopen(base + "/update/" + listname + "/" + str(index) + "/", ec)
        except HTTPError:
            bailout("Unable to update the list entry")
        
    def glob_listname(self, pattern):
        
        def match_listname(l):
            # This function doesn't seem very pythonic...
            if len(l) > 1:
                print str(len(l)) + " lists satifisfy the pattern : "
                print l

                try:
                    pat = raw_input("Enter a matching pattern to clarify your choice : ")
                except EOFError:
                    sys.exit(-1)

                return match_listname(fnmatch.filter(l, pat))
            else:
                return l[0]

        try:
            lists = self.get_lists_list()
            res = fnmatch.filter(lists, pattern)

            if len(res) > 1:
                return match_listname(res)
            else:
                return res[0]

        except HTTPError:
            bailout("Unable to glob list pattern")

def process_json_delayed_file(path, session):
    try:
        fd = open(path)
    except IOError:
        return

    data = json.load(fd)

    if "request" not in data or "listname" not in data:
        return # Discard incorrect files

    if data["request"] == "create-list":
        session.create_list(data["listname"])
    elif data["request"] == "create-entry":
        try:
            session.create_entry(data["listname"], data["content"]["title"]) 	# FIXME : use the other options if defined.
        except KeyError:
            return
    elif data["request"] == "delete-list":
        session.remove_list(data["listname"])
    elif data["request"] == "delete-entry":
        try:
            session.remove_list_entry(data["listname"], data["content"]["index"])
        except KeyError:
            return
    elif data["request"] == "update-entry":
        try:
            session.update_entry(data["listname"], data["content"]["index"], data["content"])
        except KeyError:
            return
        
    os.unlink(path)

def watch_spool_dir(name, session):
    while True:
        for file in os.listdir(os.path.expanduser('~/.liszt/delayed/')):
            process_json_delayed_file(os.path.expanduser('~/.liszt/delayed/') + file, session)
        
        time.sleep(15 * 60) # Sync the files with the server every 15 minutes.

def sync_cached_list(path, session):
    def isentrydefined(title, list):
        # Is an entry in a list ? Yes if it's title is in the list
        for e in list:
            if e["title"] == title:
                return True

        return False

    try:
        fd = open(path)
    except IOError:
        return

    cached_list = json.load(fd)
    try:
        listname = cached_list["name"]
    except KeyError:
        return

    server_list = session.get_list(listname)

    for entry in server_list:
        if isinstance(entry, list): # Discard lists for the moment
            continue
        
        cached_list[entry] = server_list[entry]

    for entry in server_list["contents"]:
        if not isentrydefined(entry["title"], cached_list["contents"]):
            cached_list["contents"].push(entry)

    json.dump(fd)
    
def watch_cache_dir(name, session):
    while True:
        for file in os.listdir(os.path.expanduser('~/.liszt/cached/')):
            sync_cached_list(os.path.expanduser('~/.liszt/cached/') + file, session)
        
        time.sleep(20 * 60) # Sync the files with the server every 20 minutes.
    
if __name__ == "__main__":

    session = LisztSession()
    session.login(username, password)

    session = ""
    thread.start_new_thread(watch_spool_dir, ("watch_spool_dir", session))

    while 1:pass
